{"meta":{"title":"guowenbo1","subtitle":null,"description":"好少年光芒万丈","author":"guowenbo1","url":""},"pages":[{"title":"404","date":"2020-11-21T07:34:10.000Z","updated":"2020-11-21T07:34:20.583Z","comments":true,"path":"404/index.html","permalink":"/404/index.html","excerpt":"","text":""},{"title":"categories","date":"2020-11-21T07:32:28.000Z","updated":"2020-11-21T07:32:56.607Z","comments":true,"path":"categories/index.html","permalink":"/categories/index.html","excerpt":"","text":""},{"title":"bangumi","date":"2019-02-10T13:32:48.000Z","updated":"2020-11-21T08:51:06.449Z","comments":false,"path":"bangumi/index.html","permalink":"/bangumi/index.html","excerpt":"","text":"","keywords":null},{"title":"about","date":"2018-12-12T14:14:36.000Z","updated":"2020-11-21T08:51:06.449Z","comments":false,"path":"about/index.html","permalink":"/about/index.html","excerpt":"","text":"[さくら荘のhojun] 与&nbsp; Mashiro&nbsp; （ 真（ま）白（しろ） ） 对话中... bot_ui_ini()","keywords":"关于"},{"title":"comment","date":"2018-12-20T15:13:48.000Z","updated":"2020-11-21T08:51:06.450Z","comments":true,"path":"comment/index.html","permalink":"/comment/index.html","excerpt":"","text":"念两句诗 叙别梦、扬州一觉。 【宋代】吴文英《夜游宫·人去西楼雁杳》","keywords":"留言板"},{"title":"client","date":"2018-12-20T15:13:35.000Z","updated":"2020-11-21T08:51:06.449Z","comments":false,"path":"client/index.html","permalink":"/client/index.html","excerpt":"","text":"直接下载 or 扫码下载：","keywords":"Android客户端"},{"title":"lab","date":"2019-01-05T13:47:59.000Z","updated":"2020-11-21T08:51:06.450Z","comments":false,"path":"lab/index.html","permalink":"/lab/index.html","excerpt":"","text":"sakura主题balabala","keywords":"Lab实验室"},{"title":"donate","date":"2018-12-20T15:13:05.000Z","updated":"2020-11-21T08:51:06.450Z","comments":false,"path":"donate/index.html","permalink":"/donate/index.html","excerpt":"","text":"","keywords":"谢谢饲主了喵~"},{"title":"rss","date":"2018-12-20T15:09:03.000Z","updated":"2020-11-21T08:51:06.451Z","comments":true,"path":"rss/index.html","permalink":"/rss/index.html","excerpt":"","text":""},{"title":"music","date":"2018-12-20T15:14:28.000Z","updated":"2020-11-21T08:51:06.451Z","comments":false,"path":"music/index.html","permalink":"/music/index.html","excerpt":"","text":"","keywords":"喜欢的音乐"},{"title":"links","date":"2018-12-19T15:11:06.000Z","updated":"2020-11-21T08:51:06.451Z","comments":true,"path":"links/index.html","permalink":"/links/index.html","excerpt":"","text":"","keywords":"友人帐"},{"title":"tags","date":"2018-12-12T14:14:16.000Z","updated":"2020-11-21T08:51:06.452Z","comments":true,"path":"tags/index.html","permalink":"/tags/index.html","excerpt":"","text":""},{"title":"theme-sakura","date":"2019-01-04T14:53:25.000Z","updated":"2020-11-21T08:51:06.452Z","comments":false,"path":"theme-sakura/index.html","permalink":"/theme-sakura/index.html","excerpt":"","text":"Hexo主题Sakura修改自WordPress主题Sakura，感谢原作者Mashiro","keywords":"Hexo 主题 Sakura 🌸"},{"title":"video","date":"2018-12-20T15:14:38.000Z","updated":"2020-11-21T08:51:06.453Z","comments":false,"path":"video/index.html","permalink":"/video/index.html","excerpt":"","text":"var videos = [ { img: 'https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg', title: '朝花夕誓——于离别之朝束起约定之花', status: '已追完', progress: 100, jp: 'さよならの朝に約束の花をかざろう', time: '放送时间: 2018-02-24 SUN.', desc: ' 住在远离尘嚣的土地，一边将每天的事情编织成名为希比欧的布，一边静静生活的伊欧夫人民。在15岁左右外表就停止成长，拥有数百年寿命的他们，被称为“离别的一族”，并被视为活着的传说。没有双亲的伊欧夫少女玛奇亚，过着被伙伴包围的平稳日子，却总感觉“孤身一人”。他们的这种日常，一瞬间就崩溃消失。追求伊欧夫的长寿之血，梅萨蒂军乘坐着名为雷纳特的古代兽发动了进攻。在绝望与混乱之中，伊欧夫的第一美女蕾莉亚被梅萨蒂带走，而玛奇亚暗恋的少年克里姆也失踪了。玛奇亚虽然总算逃脱了，却失去了伙伴和归去之地……。' }, { img : 'https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg', title: '朝花夕誓——于离别之朝束起约定之花', status: '已追完', progress: 100, jp: 'さよならの朝に約束の花をかざろう', time: '2018-02-24 SUN.', desc: ' 住在远离尘嚣的土地，一边将每天的事情编织成名为希比欧的布，一边静静生活的伊欧夫人民。在15岁左右外表就停止成长，拥有数百年寿命的他们，被称为“离别的一族”，并被视为活着的传说。没有双亲的伊欧夫少女玛奇亚，过着被伙伴包围的平稳日子，却总感觉“孤身一人”。他们的这种日常，一瞬间就崩溃消失。追求伊欧夫的长寿之血，梅萨蒂军乘坐着名为雷纳特的古代兽发动了进攻。在绝望与混乱之中，伊欧夫的第一美女蕾莉亚被梅萨蒂带走，而玛奇亚暗恋的少年克里姆也失踪了。玛奇亚虽然总算逃脱了，却失去了伙伴和归去之地……。' } ] .should-ellipsis{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;width:95%;}.should-ellipsis-full{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;width:100%;}.should-ellipsis i{position:absolute;right:24px;}.grey-text{color:#9e9e9e !important}.grey-text.text-darken-4{color:#212121 !important}html{line-height:1.15;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}img{border-style:none}progress{display:inline-block;vertical-align:baseline}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}html{-webkit-box-sizing:border-box;box-sizing:border-box}*,*:before,*:after{-webkit-box-sizing:inherit;box-sizing:inherit}ul:not(.browser-default){padding-left:0;list-style-type:none}ul:not(.browser-default)>li{list-style-type:none}.card{-webkit-box-shadow:0 2px 2px 0 rgba(0,0,0,0.14),0 3px 1px -2px rgba(0,0,0,0.12),0 1px 5px 0 rgba(0,0,0,0.2);box-shadow:0 2px 2px 0 rgba(0,0,0,0.14),0 3px 1px -2px rgba(0,0,0,0.12),0 1px 5px 0 rgba(0,0,0,0.2)}.hoverable{-webkit-transition:-webkit-box-shadow .25s;transition:-webkit-box-shadow .25s;transition:box-shadow .25s;transition:box-shadow .25s,-webkit-box-shadow .25s}.hoverable:hover{-webkit-box-shadow:0 8px 17px 0 rgba(0,0,0,0.2),0 6px 20px 0 rgba(0,0,0,0.19);box-shadow:0 8px 17px 0 rgba(0,0,0,0.2),0 6px 20px 0 rgba(0,0,0,0.19)}i{line-height:inherit}i.right{float:right;margin-left:15px}.bangumi .right{float:right !important}.material-icons{text-rendering:optimizeLegibility;-webkit-font-feature-settings:'liga';-moz-font-feature-settings:'liga';font-feature-settings:'liga'}.row{margin-left:auto;margin-right:auto;margin-bottom:20px}.row:after{content:\"\";display:table;clear:both}.row .col{float:left;-webkit-box-sizing:border-box;box-sizing:border-box;padding:0 .75rem;min-height:1px}.row .col.s12{width:100%;margin-left:auto;left:auto;right:auto}@media only screen and (min-width:601px){.row .col.m6{width:50%;margin-left:auto;left:auto;right:auto}}html{line-height:1.5;font-family:-apple-system,BlinkMacSystemFont,\"Segoe UI\",Roboto,Oxygen-Sans,Ubuntu,Cantarell,\"Helvetica Neue\",sans-serif;font-weight:normal;color:rgba(0,0,0,0.87)}@media only screen and (min-width:0){html{font-size:14px}}@media only screen and (min-width:992px){html{font-size:14.5px}}@media only screen and (min-width:1200px){html{font-size:15px}}.card{position:relative;margin:.5rem 0 1rem 0;background-color:#fff;-webkit-transition:-webkit-box-shadow .25s;transition:-webkit-box-shadow .25s;transition:box-shadow .25s;transition:box-shadow .25s,-webkit-box-shadow .25s;border-radius:2px}.card .card-title{font-size:24px;font-weight:300}.card .card-title.activator{cursor:pointer}.card .card-image{position:relative}.card .card-image img{display:block;border-radius:2px 2px 0 0;position:relative;left:0;right:0;top:0;bottom:0;width:100%}.card .card-content{padding:24px;border-radius:0 0 2px 2px}.card .card-content p{margin:0}.card .card-content .card-title{display:block;line-height:32px;margin-bottom:8px}.card .card-content .card-title i{line-height:32px}.card .card-reveal{padding:24px;position:absolute;background-color:#fff;width:100%;overflow-y:auto;left:0;top:100%;height:100%;z-index:3;display:none}.card .card-reveal .card-title{cursor:pointer;display:block}.waves-effect{position:relative;cursor:pointer;display:inline-block;overflow:hidden;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-tap-highlight-color:transparent;vertical-align:middle;z-index:1;-webkit-transition:.3s ease-out;transition:.3s ease-out}.waves-effect img{position:relative;z-index:-1}.waves-block{display:block}::-webkit-input-placeholder{color:#d1d1d1}::-moz-placeholder{color:#d1d1d1}:-ms-input-placeholder{color:#d1d1d1}::-ms-input-placeholder{color:#d1d1d1}[type=\"radio\"]:not(:checked){position:absolute;opacity:0;pointer-events:none}[type=\"radio\"]:not(:checked)+span{position:relative;padding-left:35px;cursor:pointer;display:inline-block;height:25px;line-height:25px;font-size:1rem;-webkit-transition:.28s ease;transition:.28s ease;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}[type=\"radio\"]:not(:checked)+span:before,[type=\"radio\"]:not(:checked)+span:after{border-radius:50%}[type=\"radio\"]:not(:checked)+span:before,[type=\"radio\"]:not(:checked)+span:after{border:2px solid #5a5a5a}[type=\"radio\"]:not(:checked)+span:after{-webkit-transform:scale(0);transform:scale(0)}[type=\"checkbox\"]:not(:checked){position:absolute;opacity:0;pointer-events:none}[type=\"checkbox\"]:not(:checked):disabled+span:not(.lever):before{border:none;background-color:rgba(0,0,0,0.42)}[type=\"checkbox\"].filled-in:not(:checked)+span:not(.lever):before{width:0;height:0;border:3px solid transparent;left:6px;top:10px;-webkit-transform:rotateZ(37deg);transform:rotateZ(37deg);-webkit-transform-origin:100% 100%;transform-origin:100% 100%}[type=\"checkbox\"].filled-in:not(:checked)+span:not(.lever):after{height:20px;width:20px;background-color:transparent;border:2px solid #5a5a5a;top:0px;z-index:0}input[type=checkbox]:not(:disabled) ~ .lever:active:before,input[type=checkbox]:not(:disabled).tabbed:focus ~ .lever::before{-webkit-transform:scale(2.4);transform:scale(2.4);background-color:rgba(0,0,0,0.08)}input[type=range].focused:focus:not(.active)::-webkit-slider-thumb{-webkit-box-shadow:0 0 0 10px rgba(38,166,154,0.26);box-shadow:0 0 0 10px rgba(38,166,154,0.26)}input[type=range].focused:focus:not(.active)::-moz-range-thumb{box-shadow:0 0 0 10px rgba(38,166,154,0.26)}input[type=range].focused:focus:not(.active)::-ms-thumb{box-shadow:0 0 0 10px rgba(38,166,154,0.26)} 番组计划 这里将是永远的回忆 window.onload = function(){ videos.forEach(function(video, i){ $('#rootRow').append(` ${video.title} ${video.jp} ${video.status} ${video.title} ${video.jp} 放送时间: ${video.time} ${video.desc} ${video.status} `) }) }","keywords":"B站"}],"posts":[{"title":"Jsdelivr的使用","slug":"Jsdelivr的使用","date":"2020-11-26T12:54:54.000Z","updated":"2020-11-26T14:10:34.052Z","comments":true,"path":"2020/11/26/Jsdelivr的使用/","link":"","permalink":"/2020/11/26/Jsdelivr的使用/","excerpt":"","text":"Jsdelivr# add / at the end to get a directory listing &gt; https://cdn.jsdelivr.net/gh/guowenbo1/cdn@main/ # 在原先的url中，将“https://cdn.jsdelivr.net/…” 替换为 “https://purge.jsdelivr.net/…” 即可刷新 &gt; https://purge.jsdelivr.net/gh/guowenbo1/cdn@main/ Git 基本操作git status # 查看工作区代码相对于暂存区的差别 git add . # 将当前目录下修改的所有代码从工作区添加到暂存区 . 代表当前目录 git commit -m # ‘注释’ 将缓存区内容添加到本地仓库 git pull origin master # 先将远程仓库master中的信息同步到本地仓库master中 git push origin master # 将本地版本库推送到远程服务器， # origin是远程主机，master表示是远程服务器上的master分支和本地分支重名的简写，分支名是可以修改的 # 先add再commit再push","categories":[{"name":"技术","slug":"技术","permalink":"/categories/技术/"}],"tags":[],"keywords":[{"name":"技术","slug":"技术","permalink":"/categories/技术/"}]},{"title":"逻辑回归","slug":"逻辑回归","date":"2020-11-26T07:26:14.000Z","updated":"2020-11-26T14:22:27.711Z","comments":true,"path":"2020/11/26/逻辑回归/","link":"","permalink":"/2020/11/26/逻辑回归/","excerpt":"","text":"","categories":[{"name":"python","slug":"python","permalink":"/categories/python/"}],"tags":[],"keywords":[{"name":"python","slug":"python","permalink":"/categories/python/"}]},{"title":"PCA","slug":"PCA","date":"2020-11-26T06:54:29.000Z","updated":"2020-11-26T14:22:39.539Z","comments":true,"path":"2020/11/26/PCA/","link":"","permalink":"/2020/11/26/PCA/","excerpt":"","text":"PCA降维数据集secom.data包含有590个特征，几乎所有样本都有NaN。利用PCA对该数据进行降维。实验要求：1）将所有NaN替换成该列特征的平均值；2）根据上面数据集，给出标准化后的数据，该矩阵均值为0，方差为1；3）计算并输出该矩阵的协方差矩阵；4）计算并输出协方差矩阵的特征值和特征向量；5）计算并输出贡献率和累计贡献率，取累计贡献大于85%的特征向量；6）利用该特征向量，计算降维后的数据，并将该数据保存在compress_data.xls中；7）绘制主成分与方差变化图 因此次作业最开始没有想用np,array，用了嵌套列表导致时间复杂度过高以至于进行矩阵相乘时会卡死，之后不得以而用了nparray，导致整体结构混乱。 通过此次试验的练习，了解了 - Np。Array的基本功能及运算 - PCA的功能及流程 - Python里NaN这一特殊数据类型为float - 协方差矩阵，特征值和特征向量在大量数据里代表的含义。 import math import numpy as np import xlrd import pandas as pd import xlwt # &quot;{:+.10f}&quot;.format 返回值位str #np.array 比嵌套列表计算的效率要高的多 # 把list里的缺失数据替换为该list平均值 and modify all value in list to float def insert_nulldata(list): temp_list = [] # temp list to store non NAN value for i in list: if float(i) == float(i): # non NAN value temp_list.append(float(&quot;{:.7f}&quot;.format(float(i)))) # 保留8位小数 average_value = average(temp_list) # use average function to calculate avaerage value for i in range(len(list)): if float(list[i]) != float(list[i]): # IF list[i] is NAN value list[i] = average_value else: list[i] = float(&quot;{:.7f}&quot;.format(float(list[i]))) return list ## to calcualte average value on list def average(list): denom = len(list) sum = 0 for i in list: sum = float(sum) + float(i) value = float(&quot;{:.7f}&quot;.format(sum / denom)) # 保留8位小数 return value # 标准差可能为0 data_column[5]此列向量全为100 # 返回值类型为&quot;numpy.ndarray&quot;？？？？？ def normalization_standard_value(list, average_value): numer = 0 temp_list = [] for i in list: difference = float(i) - float(average_value) numer = numer + difference ** 2 denom = len(list) - 1 standard = math.sqrt(numer / denom) # standard value if standard != 0: for i in range(len(list)): norm_standard = (float(list[i]) - float(average_value)) / standard list[i] = (float(&quot;{:.7f}&quot;.format(float(norm_standard)))) # for i in range(len(list)): # # list[i] = float(list[i]) ###list[i]扔为str类型 temp_list = list.astype(float) #astype不改变源数据类型 要赋值给其他变量才会存储下来 return temp_list # 计算协方差，输入为两个行向量 def cov(row1, row2): aver_1 = average(row1) aver_2 = average(row2) numer = 0 for i in range(len(row1)): x = (float(row1[i]) - aver_1) * (float(row2[i]) - aver_2) numer = numer + x denom = len(row1) - 1 value = round(numer / denom, 10) return value #计算主成分的个数 输入为相关系数矩阵的特征值 def cal_main_com(eigList): sum1 = 0 for i in range(len(eigList)): if sum1 &lt;= 0.85: sum1 = eigValsList[i] / sum(eigValsList) + sum1 else: return i #降维函数 参数为需要降参的列表,特征向量，主成分个数 def reduce_dimen(list,eiglist,main_com): list1 = [] for i in range(len(list)): ##590 temp_list = [] sum = 0 for j in range(main_com): #109 value = sum + list[i] *eiglist[i][j] temp_list.append(value) list1.append(temp_list) return list1 #dtype = U10 意思10个unicode string的字符？？ if __name__ == &quot;__main__&quot;: np.set_printoptions(suppress=True) file_input_address = &quot;实验3/secom.data&quot; with open(file_input_address) as fo: b = fo.readlines() # 返回值为list 每个值由每行构成的字符组成 type(b(0)) = str row = len(b) # 总行数 从0开始 data = [] # 嵌套列表 data[0]为第一行数据 for i in b: i_array = i.split(&quot; &quot;) data.append(i_array) data_column = np.transpose(data) # 嵌套列表 data_column[0]为第一列数据 并且没有缺失值 z_list = [] #嵌套列表 存储标准化的数据 类型为LIST for i in range(len(data_column)): data_column[i] = insert_nulldata(data_column[i]) ##缺失值填充 执行590次 z_list.append(normalization_standard_value(data_column[i], average(data_column[i]))) for i in range(len(z_list)): print(&quot;第&quot;,i,&quot;列标准化数据为&quot;,z_list[i]) z_matrix = np.matrix(z_list) covMatrics =np.cov(z_matrix) #相关系数矩阵 print(&quot;covmartics is&quot;,covMatrics) ##eigVals返回值类型为np.ndarray len为590; # eigVects返回值类型为numpy.matrix ndim属性为2 tupe为590 590 eigVals, eigVects = np.linalg.eig(np.mat(covMatrics)) print(&quot;协方差矩阵的特征值为&quot;,eigVals,&quot;特征向量为&quot;,eigVects) #转换成列表 eigValsList = eigVals.tolist() eigVectsList = eigVects.tolist() #将特征值从大到小排序，特征向量根据特征值的位置排序 eigValsList,eigVectsList = (list(t) for t in zip(*sorted(zip(eigValsList,eigVectsList),reverse=True))) main_com = cal_main_com(eigValsList) print(&quot;前&quot;,main_com,&quot;个主成分累计贡献率已经超过85%&quot;) #降维 finalise_vector = reduce_dimen(z_list,eigVectsList,main_com) #嵌套列表 存储降维后的数据 109行 590列 print(&quot;降维后的第一个数据为&quot;,finalise_vector[0]) # 写入xls workbook = xlwt.Workbook(encoding=&#39;utf-8&#39;) worksheet = workbook.add_sheet(&#39;sheet1&#39;) b = 0 for i in finalise_vector: worksheet.write(b, 0, i) b = b + 1 workbook.save(&#39;compress_data.xls&#39;) # 保存文件 # ##5分钟都没算出来协方差矩阵 # relation_matrix = [] # for i in range(len(z_list)): # temp_list = [] # for j in range(len(z_list)): # value = cov(z_list[j],z_list[i]) # temp_list.append(value) # relation_matrix.append(temp_list) # # print(&quot;cov martic is&quot;,relation_matrix)","categories":[{"name":"python","slug":"python","permalink":"/categories/python/"}],"tags":[],"keywords":[{"name":"python","slug":"python","permalink":"/categories/python/"}]},{"title":"数据预处理","slug":"数据预处理","date":"2020-11-26T06:15:46.000Z","updated":"2020-11-26T14:23:01.437Z","comments":true,"path":"2020/11/26/数据预处理/","link":"","permalink":"/2020/11/26/数据预处理/","excerpt":"","text":"实验二、数据预处理1：缺失数据填充餐饮系统中的销售数据可能会出现缺失，catering_sale_missing.xls为某餐厅一段时间的销量，其中，有部分数据缺失。利用拉格朗日插值对缺失数据进行插补。要求： - 前后取值个数k=3 - 读出并显示原始数据 - 插值后的数据另存为missing_data_processing.xls - 读出并显示插值后数据 2：相关性分析利用missing_data_processing.xls，通过Python中统计函数，分析这些菜品销售之间的相关性。要求： - 计算任意两款菜式之间的相关系数 - 计算“百合酱蒸凤爪”与其他菜式的相关系数 - 计算“百合酱蒸凤爪”与“翡翠蒸香茜饺”的相关系数 3：数据规范化使用python 相关库完成给定数据集的据规范化和转换。要求： - 读取normalization_data.xls - 最小-最大规范化数据 - Z-score规范化(零-均值规范化) - 小数定标规范化 - 输出所有规范化结果 4：数据离散化- 读取discretization_data.xls，使用padans.cut()对数据进行0-3的离散处理； - 处理后的数据保存在data_descret.xls中 5：数据转化- 读取sales_data.xls，将字符数据转换成数值型数据，并将转换结果保存为data_tras.xls（data_transfer） 缺失数据补充import xlrd import pandas as pd from scipy.interpolate import lagrange import numpy as np or_file_ad = &quot;catering_sale_missing.xls&quot; #文件地址 or_file = xlrd.open_workbook(or_file_ad) #打开文件 sheet1 = or_file.sheet_by_index(0) #第一个工作表 org_data = sheet1.col_values(1) #所需要的数据，为第一个工作表的第二列除第一行之外的值 org_data.remove(&#39;销量&#39;) #移除销量 out_files = &#39;missing_data_processing.xls&#39; #作业要求2：读出并显示原始数据 print(org_data) #s和n为列向量 k为要参考数据的个数 def polyinterp_column(s, n, k=3): y = s.iloc[list(range(n-k,n)) + list(range(n + 1, n + 1 + k))] y = y[y.notnull()] return lagrange(y.index,list(y))(n) #读入数据 data = pd.read_excel(or_file_ad) #进行拉格朗日插值法 for i in data.columns: for j in range(len(data)): if (data[i].isnull())[j]: data.loc[j,i] = polyinterp_column(data[i],j) # data_vector = np.asarray(org_data) # for i in range(len(org_data)): # if (org_data[i]==&#39;&#39;): #判断是否为空 由于是字符型不能用None # org_data[i] = polyinterp_column(data_vector,i) #写出数据 data.to_excel(out_files) 数据规范化import xlrd import numpy as np file_ad = &quot;normalization_data.xls&quot; # 文件地址 file_data = xlrd.open_workbook(file_ad) # 打开文件 sheet1 = file_data.sheet_by_index(0) # 第一个工作表 all_rows = sheet1.nrows # 总行数 all_col = sheet1.ncols # 总列数 data = [] # 存储数据的列表 for i in range(all_rows): for k in range(all_col): data.append(sheet1.cell(i, k).value) # # 最小-最大规范化 min当前数据集的最小值 max当前数据集的最大值 values当前要规范的数据 # 此为0-1规范化 规范化后的数据为0~1 def min_max(min, max, values): num = values - min dom = max - min data = (num / dom) * (1 - 0) + 0 return data # z分数规范化 average为数据集的平均值 std为数据集的标准差 value为当前要规范的数据 def z_score(average, std, value): data = (value - average) / std return data # 小数定标规范化 j为10为底values绝对值最大的对数 def dec_norm(j, value): data = value / 10 ** j return data # 最小最大规范化数据 max1 = max(data) min1 = min(data) data_minmax_norm = [] for i in data: data_minmax_norm.append(min_max(min1, max1, i)) print(&quot;最小-最大（0-1)规范化数据为：&quot;, data_minmax_norm) # z分数规范化 average1 = np.mean(data) std1 = np.std(data, ddof=1) data_zscore_norm = [] for i in data: data_zscore_norm.append((z_score(average1, std1, i))) print(&quot;z分数规范化数据为：&quot;, data_zscore_norm) # 小数定标规范化 # max_abs为绝对值最大的数 if abs(min1) &gt; abs(max1): max_abs = min1 else: max_abs = max1 j = np.log10(max_abs) data_dec_norm = [] for i in data: data_dec_norm.append(dec_norm(j, i)) print(&quot;小数定标规范化数据为：&quot;, data_dec_norm) 数据离散化import xlrd import pandas as pd import xlwt file_ad = &quot;discretization_data.xls&quot; #文件地址 file_data = xlrd.open_workbook(file_ad) #打开文件 sheet1 = file_data.sheet_by_index(0) #第一个工作表 all_rows = sheet1.nrows #总行数 all_col = sheet1.ncols #总列数 out_files = &#39;data_descret.xls&#39; data = [] #存储数据的列表 for i in range(all_rows): for k in range(all_col): data.append(sheet1.cell(i,k).value) data.remove(&#39;肝气郁结症型系数&#39;) #移除label #离散化 descret_data = pd.cut(data,3,labels=False) #写入xls workbook = xlwt.Workbook(encoding =&#39;utf-8&#39;) worksheet = workbook.add_sheet(&#39;sheet1&#39;) b = 0 for i in descret_data: worksheet.write(b,0,int(i)) b = b + 1 workbook.save(&#39;data_descret.xls&#39;) #保存文件 数据转换import xlrd import xlwt file_ad = &quot;sales_data.xls&quot; #文件地址 file_data = xlrd.open_workbook(file_ad) #打开文件 sheet1 = file_data.sheet_by_index(0) #第一个工作表 all_rows = sheet1.nrows #总行数 从1开始 all_col = sheet1.ncols #总列数 从1开始 a = 0 label = [] #存储第一行的值 for i in range(all_col): label.append(sheet1.cell(0,i).value) data = [] #存储数据的列表 不包含第一行及序号列 for i in range(1,all_rows): # 1开始是为了去除第一行 for k in range(1,all_col): # 1开始是为了去除序号列 if sheet1.cell(i,k).value == &#39;坏&#39; or sheet1.cell(i,k).value == &#39;否&#39; or sheet1.cell(i,k).value == &#39;低&#39;: data.append(0) else: data.append(1) # 创建新的xls表 workbook = xlwt.Workbook(encoding =&#39;utf-8&#39;) worksheet = workbook.add_sheet(&#39;sheet1&#39;) #写入第一行 b = 0 for i in label: worksheet.write(0,b,i) b = b + 1 #写入序号列 c = 1 for i in range((all_rows-1)): worksheet.write(c,0,int(i+1)) c = c + 1 #写入转换后的值 #worksheet.write里的单元格从0开始 for i in range(2,all_rows + 1): for k in range(2,all_col + 1): x = (i - 2) * (all_col - 1) + (k - 1) worksheet.write((i-1),(k-1),data[x-1]) workbook.save(&#39;data_trassfer.xls&#39;) #保存文件 相关性分析import xlrd file_ad = &quot;catering_sale_all.xls&quot; #文件地址 file_data = xlrd.open_workbook(file_ad) #打开文件 sheet1 = file_data.sheet_by_index(0) #第一个工作表 all_rows = sheet1.nrows #总行数 all_col = sheet1.ncols #总列数 a = 0 label = [] #存储第一行的值（菜系名字） for i in range(1,all_col): label.append(sheet1.cell(0,i).value) data = [] #存储数据的列表 不包含时间以及菜名 for i in range(1,all_rows): # 1开始是为了去除菜名 for k in range(1,all_col): # 1开始是为了去除时间 data.append(sheet1.cell(i,k).value) ## 斯皮尔慢相关系数 输入为两个列表 输出为数值型 def spearman(lst1,lst2): n = len(lst1) num = 0 #分子 for i in range(n): num = num + ((lst1[i] - lst2[i]) ** 2) value = 1 - ((6 * num) / (n * ((n * n) - 1))) return value ##双重循环判断各个菜系的spearman相关系数 for a in label: lst1 = [] #每次a循环后给lst1清空 否则会遗留上次循环的数据 for i in range(label.index(a), len(data), len(label)): #循环找到data里a的销售数据 if data[i] == &quot;&quot;: #缺失值赋空值 data[i] = 0 lst1.append(data[i]) #a的销售数据的列表 for b in label: lst2 = [] for i in range(label.index(b), len(data), len(label)): #循环找到data里b的销售数据 if data[i] == &quot;&quot;: #缺失值赋空值 data[i] = 0 lst2.append(data[i]) #b的销售数据的列表 corelation_value = spearman(lst1,lst2) print(a,&#39;与&#39;,b,&#39;spearman的相关系数为:&#39;,corelation_value)","categories":[{"name":"python","slug":"python","permalink":"/categories/python/"}],"tags":[],"keywords":[{"name":"python","slug":"python","permalink":"/categories/python/"}]},{"title":"TF-IDF","slug":"TF-IDF","date":"2020-11-23T12:12:29.000Z","updated":"2020-11-26T14:22:46.616Z","comments":true,"path":"2020/11/23/TF-IDF/","link":"","permalink":"/2020/11/23/TF-IDF/","excerpt":"","text":"1、给定中文停用词库stopwords, 利用jieba对语料库中的文档进行中文分词，并将分词存入分词文档；2、利用TF-IDF算法对已分词的文档，采用余弦相似方法，计算doc1.txt与其它各文档之间的相似性（文档见word2019）；3、要求：1）安装jieba2）提供源码3）完成实验报告 import jieba import math import cmath word_all = [] # 存储所有分词的列表 file_list = [&quot;doc1.txt&quot;, &quot;doc2.txt&quot;, &quot;doc3.txt&quot;, &quot;doc4.txt&quot;, &quot;doc5.txt&quot;] dic_list = [&quot;d1&quot;, &quot;d2&quot;, &quot;d3&quot;, &quot;d4&quot;, &quot;d5&quot;] # 存放去除停用词后的分词表 dic_list[0]为第一个文档的分词表 tfidf_list = [&quot;a1&quot;, &quot;a2&quot;, &quot;a3&quot;, &quot;a4&quot;, &quot;a5&quot;] with open(&quot;CNENstopwords.txt&quot;, encoding=&quot;utf8&quot;) as fo: stopwords = fo.read() def count_words(txt): # 返还已删除停用词之后词出现的频率 返还值为词典 with open(txt, &quot;r&quot;, encoding=&quot;utf8&quot;) as file: dic = {} # 局部变量dic text = [] doc = file.read() cuttest = jieba.lcut(doc) for i in cuttest: # text为删除停用词后的分词表 if i not in stopwords: text.append(i) for word in text: if word not in dic: dic[word] = 1 else: dic[word] = dic[word] + 1 text.clear() for i in dic: word_all.append(i) return dic # 返还值最好用局部变量 b = 0 for i in file_list: #遍历文档列表 返还词典dic[list] 为已删除停用词之后词出现的频率 dic_list[b] = count_words(i) b = b + 1 # 统计该词在5个文档中出现的频率 def dic_all(word_list): dic_all = {} for i in word_list: if str(i) in dic_all: dic_all[str(i)] = dic_all[str(i)] + 1 else: dic_all[str(i)] = 1 return dic_all a = dic_all(word_all) def IDF(dic): dic_idf = {} for key, value in dic.items(): a = len(file_list) / (value + 1) idf = math.log(a) dic_idf[key] = idf return dic_idf idf = IDF(a) #列表idf存储 def TF_IDF(dic, idf): # 计算tf_idf 返还值为字典 word_dic = {} for key, value in idf.items(): if key in dic: word_dic[key] = dic[key] * idf[key] else: word_dic[key] = 0 return word_dic a = 0 for i in tfidf_list: # 遍历每个文档分词表 返回tf-idf值 tfidf_list[a] = TF_IDF(dic_list[a], idf) a = a + 1 def sim(dicx, dicy): #计算余弦相似度 numerator = 0 x = 0 y = 0 deno = 0 for i in dicx: numerator = numerator + dicx[i] * dicy[i] # 分子 x = x + dicx[i] * dicx[i] y = y + dicy[i] * dicy[i] deno = cmath.sqrt(x) * cmath.sqrt(y) # 分母 value = numerator / deno return value for i in range(1,5): print(&quot;第一个文档与第&quot;,str(i+1),&quot;个文档余弦相似度为&quot;, sim(tfidf_list[0], tfidf_list[i]))","categories":[{"name":"python","slug":"python","permalink":"/categories/python/"}],"tags":[],"keywords":[{"name":"python","slug":"python","permalink":"/categories/python/"}]},{"title":"命令行的使用","slug":"命令行的使用","date":"2020-11-21T09:25:46.000Z","updated":"2020-11-23T12:12:11.898Z","comments":true,"path":"2020/11/21/命令行的使用/","link":"","permalink":"/2020/11/21/命令行的使用/","excerpt":"","text":"Git CMD(Windows)cd 切换目录 md 创建目录 rd 删除目录 Git Bash(Linux)pwd # 显示当前目录 ls #展示文件夹内容 ls -l #串行展示文件夹内容 常用 cd .. #退后 vim #打开文件 :wq #修改文件时使用 保存并退出 cd /root/Docements # 切换到目录/root/Docements cd ./path # 切换到当前目录下的path目录中，“.”表示当前目录 cd ../path # 切换到上层目录中的path目录中，“..”表示上一层目录","categories":[{"name":"技术","slug":"技术","permalink":"/categories/技术/"}],"tags":[],"keywords":[{"name":"技术","slug":"技术","permalink":"/categories/技术/"}]},{"title":"hexo","slug":"hexo","date":"2020-11-21T09:06:37.000Z","updated":"2020-11-23T12:01:52.701Z","comments":true,"path":"2020/11/21/hexo/","link":"","permalink":"/2020/11/21/hexo/","excerpt":"","text":"hexo的命令hexo s #启动hexo hexo n #新建博客 hexo clean #清理 hexo g #生成 hexo d #部署 连接github和hexo安装布置包 cnpm install --save hexo-deployer-git 2修改blog目录下的config在config里的deployment添加 type：git 和仓库地址和branch类型 type: git repo: https://github.com/guowenbo1/guowenbo1.github.io branch: master hexo的使用自己的使用流程​ 先用hexo n 新建博客并修改​ 依次运行 hexo clean； hexo g； hexo s​ 这样本地localhost就会更新​ 运行hexo d 使https://guowenbo1.github.io/更新​ 疑问 ：让github里hexo博客更新的方法是 hexo d？ 直接 hexo d 遇见的错误及解决方法运行hexo d时提示 ERROR Deployer not found: git npm install --save hexo-deployer-git 即可","categories":[{"name":"技术","slug":"技术","permalink":"/categories/技术/"}],"tags":[],"keywords":[{"name":"技术","slug":"技术","permalink":"/categories/技术/"}]},{"title":"oracle","slug":"oracle","date":"2020-11-21T09:06:31.000Z","updated":"2020-11-23T12:11:52.984Z","comments":true,"path":"2020/11/21/oracle/","link":"","permalink":"/2020/11/21/oracle/","excerpt":"","text":"SQL查询向Course表中插入一行 insert into course values(&#39;C50&#39;,&#39;Oracle&#39;,&#39;&#39;,&#39;4&#39;,&#39;100&#39;); 使用Insert语句为CS系学生学生选择必修课C50 INSERT INTO sc SELECT SNO,&#39;C50&#39;,NULL FROM STUDENT WHERE SDEPT=&#39;CS&#39;; 将Oracle课程的先行课修改为“DB”课程的课程号（使用子查询完成 UPDATE COURSE SET CPNO=(select cno from course where cname=&#39;DB&#39;) WHERE Cname=&#39;Oracle&#39;; 将CS系”DB”课程成绩为空的学生选课信息删除 delete from sc where Grade is null and cno in ( select cno from course where cname=&#39;DB&#39; ) and Sno in ( select sno from student where sdept=&#39;CS&#39; ); 查询CS系没有选’DB’课程的学生的学号，姓名，系 select sno,sname,sdept from Student where Sdept = &#39;CS&#39; and not exists ( select * from SC,Course where Student.Sno = SC.Sno and Course.Cno = SC.Cno and Course.Cname = &#39;DB&#39; ); 查询男同学选择了’DB’课程，但是没有选’Oracle’课程的学生，列出学号，姓名 Select Sno,Sname from Student where SSex = &#39;m&#39; and Not Exists( Select * from Course,SC where Course.Cno = SC.Cno and Student.Sno = SC.Sno and Course.Cname = &#39;Oracle&#39; ) and Exists( Select * from Course,SC where Course.Cno = SC.Cno and Student.Sno = SC.Sno and Course.Cname = &#39;DB&#39; ); 查询选课人数大于等于3人的课程，列出课程号，课程名，选课人数（scnt）,并按课程号升序排列 SELECT Course.Cno,Course.Cname,count(sc.cno) scnt from course join SC on course.Cno=SC.cno group by course.Cno,course.cname,sc.cno having count(sc.Cno) &gt;= 3 order by sc.Cno; 查询选课人数最多的课程。列出课程号，课程名。 Select C.Cno,C.Cname from Course C,SC where C.Cno = SC.CNO group by C.Cno,C.Cname having count(C.Cno)&gt;=all( Select count(*) from Course C,SC where C.CNO = SC.CNO group by C.CNO ); 查询CS系选课人数最多的课程。列出课程号，课程名，CS系的选课人数（CScnt） select C.cno, C.cname, count(C.cname) CScnt from course C ,student S,sc where S.Sdept = &#39;CS&#39; and C.cno = sc.cno and S.sno = sc.sno group by C.cno, C.cname,s.sdept having count(c.cname) &gt;= all( select count(*) from course C, student S, sc where S.Sdept = &#39;CS&#39; and C.cno = sc.cno and S.sno = sc.sno group by C.cname ); SQL匿名块将年龄最大的同学的学号，姓名，年龄打印出来。（注：年龄最大的同学只有一个） DECLARE v_sno VARCHAR2(10); v_sname VARCHAR2(10); v_sage NUMBER; v_ssex VARCHAR2(10); BEGIN SELECT sno,sname,SAGE,SSEX INTO v_sno,v_sname,v_sage,v_ssex from STUDENT where SAGE = (select max(SAGE) from STUDENT); if v_ssex = &#39;f&#39; then v_ssex := &#39;美女&#39;; else v_ssex := &#39;帅哥&#39;; end if; DBMS_OUTPUT.PUT_LINE(trim(v_sno)||&#39;,&#39;||v_sname||&#39;,&#39;||v_sage||&#39;,&#39;||v_ssex); commit; END; 将‘004’号同学的年龄改为18岁，系别改为‘E’，为CS系年龄最小的同学选‘C01’号课程 DECLARE v_sage NUMBER:=18; v_sdept VARCHAR2(10):=&#39;E&#39;; v_cno VARCHAR2(10):=&#39;C01&#39;; BEGIN UPDATE STUDENT SET SAGE = v_sage, SDEPT = v_sdept --与where是一体的 不能加分号 WHERE sno = &#39;004&#39;; UPDATE SC set SC.cno = v_cno,SC.GRADE = NULL where ROWNUM = 1 AND EXISTS(SELECT 1 --orcale 多表关联update 不能直接加from 要where exists (select from where） from STUDENT where sc.sNO = STUDENT.sno and STUDENT.SDEPT=&#39;CS&#39; AND STUDENT.SAGE = (SELECT min(SAGE) from student WHERE SDEPT=&#39;CS&#39;) ); commit; END; 为‘002’号同学选‘C02’课程。将选课人数最多的课程的最大选课人数（snumber）改为105 DECLARE v_scno VARCHAR2(10):=&#39;C01&#39;; v_snumber VARCHAR2(10):= &#39;105&#39;; v_ssno VARCHAR2(10):=&#39;002&#39;; v_maxcno VARCHAR2(10); BEGIN insert into sc values(v_scno,v_ssno,null); SELECT c.cno into v_maxcno from course c,sc where trim(c.cno) = trim(sc.cno) group by c.cno having count(c.cno) &gt;= all( select count(*) from course c,sc where trim(c.cno) = trim(sc.cno) group by c.cno); update COURSE set SNUMBER = v_snumber where cno = v_maxcno; commit; END;","categories":[{"name":"oracle","slug":"oracle","permalink":"/categories/oracle/"}],"tags":[],"keywords":[{"name":"oracle","slug":"oracle","permalink":"/categories/oracle/"}]},{"title":"Hello World","slug":"hello-world","date":"2020-11-21T08:24:30.739Z","updated":"2020-11-26T07:28:34.899Z","comments":true,"path":"2020/11/21/hello-world/","link":"","permalink":"/2020/11/21/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post$ hexo new &quot;My New Post&quot; More info: Writing Run server$ hexo server More info: Server Generate static files$ hexo generate More info: Generating Deploy to remote sites$ hexo deploy More info: Deployment","categories":[],"tags":[],"keywords":[]}]}